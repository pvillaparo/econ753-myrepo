# Class 3 - January 27

## Purpose
Continue with optimization process 
Special Cases for optimization: tries to find update for hessian (DFP, BFGS) , but there is a more theoretical 
    way to think about hessians. We can ex-ante write down the function of the hessian. 
---

## Maximumm Likelikood 
we can use the score function as the derivative o fth eloglike at that observation data. 
        so nxk matrix: obs * vars evaluate -> the expectation of the inner product will give us the information matrix
        this new matrix will replac ethe hessian for the initial approximaiton. 
        we can code this derivate ourselves , and update the old value with the new function 

##  Nonlinear Least Squares
use for non linear model, here we define a residual function 
    there is an algotrith to update the hessian: 
        initialize 
        define jacobian (this is the score analogy)
        rewrite the gradient of the function using the jacobian 
        to get the actual hessian: inner product of jacobians + a cross terms of derivatives
        to estimate the Hessian, we dorp the second term to guarantee positive definite matrix. (no a mathematic arguments, just convenient)
        define the search step that it will be use to update the old value 
some requirements and changes: we might need to adjust the steps in smaller changes 

## Global optimization 
matlab has a global optimization toolbox, and these methods are random checks and guess methods: take initial data , guess random guess and evaluate the value , to finally decide to update randomly 
    and continue with the next step.

# Elementary Methods 
heuristics methods that can also solve problems for compelx problems

## Gauss-Jacobi 
If you have a big system of equations you can split it in smaller one . 
Imagine that x htat has the largest derivate : the most important unknown dominant in each equation -> The jacobian will compile this info
example: in io, the derivative of my own first order condition is more important that the derivative of others, (you are moer sensitive to your own variable)
Method Gauss-Jacobi
    guess an initital valueswe can solve the equations to get a new guess , get a new x_1 , f_1
    then solve for the next funciton , but only for the dominant variables (x_2, x_3...) and keep the initial x_0
    once all the equations are solves, we update the initial vector x)0
## Gauss Seidel-Method
Similar to Gauss-Jacobi but update as you move along. 

these two methods require diagonally dominat structure. 

# Numerical Methods
To approximate derivative we can use different methods:
    ## Forward Difference
    two functions evaluations at x and x+h , h can be too small creating machine isues 
    ## Central Difference
    three funciton evaluations: x, x+h, x-h 
    ## Higher order Methods
    weighting the ponts from x with more points of analysis. 

But the stepsize is crucial, too large you might not approximate properly, but poorly.

# Automatic Differentiation
compute exact derivative by applying chain rule to simple operations : construct the derivative of the function joining the small pieces (chain rule) analized
Define computation
Automatic differentiation can not be optimal when calculating BLP 

# Numerical Integration
1. Trapezoidall Rule 
    we can use linear approximation beteween points , considering nodes and weights, weights allows to consider some nodes more than others
    appproximate the integral as a line
    we point external points  less than internal points
2. Midpoint Rule
    approximate integral using rectangle
    appproximate the integral as a constant
3. Simpson's Rule 
    use a cuadratic parabola to approximate the function f(x), we need the end points and middle point. 
    parabolic approximaiton is reached using simpson's rule 
    can exactly approx the intervale of a third order polynom (theorem) , but ot exactly for other orders. 
Next methods are the ones we mostly will use in Research
## Monte Carlo Integration
Here we use (pseudo)-random nodes equally weightes
when we are using pseudo random we need to fix a seed 
relies in the strong law of large numbers  (SLLN) , not a proof-based grid because there the tails don't matter. 

```julia
using Statistics
            function mc_integral(f, a, b, n)
                # Generate random points in interval [a,b]
                x = a .+ (b-a) * rand(n)
                
                # Compute function values and scale by interval width
                y = f.(x) * (b-a)
                
                # Return mean and standard error
                return mean(y), std(y)/sqrt(n)
            end
            
            # Test with f(x) = x^2 on [0,1]
            f(x) = x^2
            n = 100000
            result, error = mc_integral(f, 0, 1, n)
            
            println("Integral of x^2 from 0 to 1:")
            println("Monte Carlo: $result Â± $error")
            println("Exact: $(1/3)")
```

## Quasi-Monte Carlo Methods 
We can use  Halto/Sobol sequence sfor better distribution. 
### Gaussian Quadrature
Here the objective is to chooce the fewest nodes possible to achive approximation
Node that for certain weights funcitons we cna find exact nodes and weights , this instead of choosing random values. 

```julia
# Define a function to calculate the factorial of a number
#cd("/Users/villap/Documents/GitHub/econ753-myrepo/lecture-notes")
#using Weave
#weave("class3-jan27.jmd", doctype = "md2html")
```
